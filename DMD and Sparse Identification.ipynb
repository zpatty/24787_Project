{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_medium():\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import glob\n",
    "    import sklearn.preprocessing\n",
    "    %matplotlib inline\n",
    "    np.set_printoptions(threshold=np.nan)\n",
    "\n",
    "    files = glob.glob(\"C:\\\\Users\\\\Zach\\\\Sync\\\\ML Class\\\\Project\\\\Data\\\\World Bank\\\\*.csv\")\n",
    "    i = 0\n",
    "    start_year = '1971'\n",
    "    end_year = '2014'\n",
    "    series = []\n",
    "    for file in files:\n",
    "        if file[-14:-4] == 'population':\n",
    "            df = pd.read_csv(file, index_col = 0)\n",
    "            series.append(file[52:-4])\n",
    "        else:\n",
    "            df = pd.read_csv(file, index_col = 0, skiprows = 4)\n",
    "            series.append(file[52:-4])\n",
    "        df.head()\n",
    "        if i == 0:\n",
    "            data = df.loc['World',start_year:end_year].reset_index().values\n",
    "            i = 1\n",
    "        elif file[-14:-4] == 'population':\n",
    "            df = df[df['Country Code'].str.match('WLD')]\n",
    "\n",
    "            data = np.concatenate((data,df[df['Year'].between(int(start_year),int(end_year))].values[:,2].reshape(-1,1)),axis = 1)\n",
    "        else:\n",
    "            data = np.concatenate((data,df.loc['World',start_year:end_year].values.reshape(-1,1)),axis = 1)\n",
    "\n",
    "    data = data[0:,1:]\n",
    "    ecopd = pd.read_csv(\"C:\\\\Users\\\\Zach\\\\Sync\\\\ML Class\\\\Project\\\\Data\\\\ecological_footprint.csv\", index_col = None)\n",
    "    ecopd = ecopd.iloc[:,2:-2].drop(['Record'], axis = 1)\n",
    "    data = np.concatenate((data[:,:], ecopd.iloc[10:,-1].values.reshape(-1,1)),axis = 1)\n",
    "    series.append('ecological footprint')\n",
    "    fossils = pd.read_csv(\"C:\\\\Users\\\\Zach\\\\Sync\\\\ML Class\\\\Project\\\\Data\\\\global-fossil-fuel-consumption.csv\", index_col = 0)\n",
    "    data = np.concatenate((data[:,:], fossils.loc[start_year:end_year,'Total'].values.reshape(-1,1)),axis = 1)\n",
    "    series.append('fossil fuels')\n",
    "    years = list(range(1971,1971+data.shape[0]))\n",
    "    data = pd.DataFrame(data = data, columns = series, index = years)\n",
    "    #data.to_csv(path_or_buf = \"C:\\\\Users\\\\Zach\\\\Sync\\\\ML Class\\\\Project\\\\Data\\\\medium.csv\" )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_big():\n",
    "    df = pd.read_csv(\"C:\\\\Users\\\\Zach\\\\Sync\\\\ML Class\\\\Project\\\\Data\\\\world_data_1.csv\")\n",
    "    df = df.loc[:,:].replace('..', np.NaN)\n",
    "    start_year = '1971'\n",
    "    end_year = '2010'\n",
    "    rows = np.where(np.prod(df.loc[:,start_year:end_year].notna().values, axis = 1))[0]\n",
    "    df = df.iloc[rows,:]\n",
    "    pred_steps = 14\n",
    "    series = df.iloc[:,2].values\n",
    "    df = df.loc[:,start_year:end_year]\n",
    "    train = df.iloc[:,:].values.astype(float)\n",
    "    years = df.loc[:,:].columns.values\n",
    "    train = pd.DataFrame(data = train.T, columns = series, index = years)\n",
    "    data = get_medium()\n",
    "    years\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import sklearn.preprocessing\n",
    "%matplotlib inline\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "data = get_medium()\n",
    "data = data[['food production','gdp','population','ecological footprint','fossil fuels']]\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "train = scaler.fit_transform(data)\n",
    "train = train.reshape(1,-1,train.shape[1])\n",
    "trainX = train[:,:30,:]\n",
    "trainY = train[:,30,:]\n",
    "#trainY = trainY.reshape(trainY.shape[1], trainY.shape[2])\n",
    "testX = train[:,42-7:42,:]\n",
    "testY = train[:,42:44,:]\n",
    "#testY = testY.reshape(testY.shape[1], testY.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (12, 8)\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from PDE_FIND import *\n",
    "import scipy.io as sio\n",
    "import itertools\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "rhs_functions = {}\n",
    "f = lambda x, y : np.prod(np.power(list(x), list(y)))\n",
    "powers = []      \n",
    "P = 2\n",
    "d = data.shape[1]\n",
    "for p in range(1,P+1):\n",
    "        size = d + p - 1\n",
    "        for indices in itertools.combinations(range(size), d-1):\n",
    "            starts = [0] + [index+1 for index in indices]\n",
    "            stops = indices + (size,)\n",
    "            powers.append(tuple(map(operator.sub, stops, starts)))\n",
    "for power in powers: rhs_functions[power] = [lambda x, y = power: f(x,y), power]\n",
    "powers = list(rhs_functions.keys())\n",
    "scaler = MinMaxScaler(feature_range=(1, 100))\n",
    "# e = data['ecological footprint'].values  # vorticity\n",
    "# p = data['population'].values/1000000000  # x-component of velocity\n",
    "# ff = data['fossil fuels'].values/1000   # y-component of velocity\n",
    "\n",
    "# ge = np.gradient(e)\n",
    "# gp = np.gradient(p)\n",
    "# gf = np.gradient(ff)\n",
    "\n",
    "# e = scaler.fit_transform(e.reshape(-1,1)).flatten()\n",
    "# p = scaler.fit_transform(p.reshape(-1,1)).flatten()\n",
    "# ff = scaler.fit_transform(ff.reshape(-1,1)).flatten()\n",
    "\n",
    "# ge = scaler.fit_transform(ge.reshape(-1,1)).flatten()\n",
    "# gp = scaler.fit_transform(gp.reshape(-1,1)).flatten()\n",
    "# gf = scaler.fit_transform(gf.reshape(-1,1)).flatten()\n",
    "\n",
    "#Y = np.concatenate((ge.reshape(-1,1), gp.reshape(-1,1), gf.reshape(-1,1)), axis = 1)\n",
    "#X_orig = np.concatenate((e.reshape(-1,1), p.reshape(-1,1), ff.reshape(-1,1)), axis = 1)\n",
    "X_orig = scaler.fit_transform(data.values)\n",
    "Y = np.gradient(X_orig, axis = 0)\n",
    "\n",
    "#print(Y)\n",
    "raised = []\n",
    "#print(len(powers))\n",
    "for power in powers:\n",
    "    raised.append(np.power(X_orig,power))\n",
    "\n",
    "X = raised\n",
    "X = np.array(X)\n",
    "X = np.prod(X,axis = 2)\n",
    "\n",
    "clf = linear_model.Lasso(alpha=1, fit_intercept = True)\n",
    "clf.fit(X.T,Y)\n",
    "#print(powers)\n",
    "coefs = clf.coef_\n",
    "#print(coefs.T)\n",
    "X = X.astype(float)\n",
    "\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "def f(y,s, coefs, powers):\n",
    "    raised = []\n",
    "    ddt_out = []\n",
    "    for power in powers:\n",
    "        raised.append(np.power(s,power))\n",
    "    X = raised\n",
    "    X = np.array(X)\n",
    "    X = np.prod(X,axis = 1)\n",
    "    ddt = coefs @ X\n",
    "    return [ddt]\n",
    "s0 = X_orig[0,:]\n",
    "t = np.linspace(1971,2040)\n",
    "#s = odeint(f,s0,t, args = (coefs,powers))\n",
    "sol = solve_ivp(lambda t, y: f(t, y, coefs,powers), [1971, 2040], s0, t_eval=t)\n",
    "print(sol)\n",
    "s = sklearn.preprocessing.scale(sol.y)\n",
    "for i in range(s.shape[0]):\n",
    "    plt.plot(t,s[i,:],'--', linewidth=2.0)\n",
    "    plt.plot(range(1971,2015),sklearn.preprocessing.scale(X_orig[:,i]))\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"S[N,C]\")\n",
    "#plt.legend([\"Predicted Ecological Footprint\",\"Predicted Population\",\"Predicted Fossil Fuels\",\"Actual Ecological Footprint\",\n",
    "#           \"Actual Population\", \"Actual Fossil Fuels\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhs_functions = {}\n",
    "f = lambda x, y : np.prod(np.power(list(x), list(y)))\n",
    "powers = []      \n",
    "P = 3\n",
    "d = 3\n",
    "for p in range(1,P+1):\n",
    "        size = d + p - 1\n",
    "        for indices in itertools.combinations(range(size), d-1):\n",
    "            starts = [0] + [index+1 for index in indices]\n",
    "            stops = indices + (size,)\n",
    "            powers.append(tuple(map(operator.sub, stops, starts)))\n",
    "for power in powers: rhs_functions[power] = [lambda x, y = power: f(x,y), power]\n",
    "coeffs = list(rhs_functions.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "for i in range(data.shape[1]):\n",
    "    Y = train[:,i]\n",
    "    print(data.columns.values[i])\n",
    "    dropped = data.drop(data.columns[i], axis = 1)\n",
    "    X = np.delete(train, i, axis = 1)\n",
    "    clf = linear_model.Lasso(alpha=0.1, fit_intercept = False)\n",
    "    clf.fit(X,Y)\n",
    "    \n",
    "    \n",
    "    print(dropped.columns.values[np.nonzero(clf.coef_)[0]])\n",
    "    print(clf.coef_[np.nonzero(clf.coef_)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing\n",
    "\n",
    "train = sklearn.preprocessing.scale(small_data)\n",
    "for i in range(train.shape[1]-1):\n",
    "    plt.plot(range(1971,train.shape[0]+1971),train[:,i].astype(np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train = data.values.astype(np.float64)\n",
    "U, s, Vh = np.linalg.svd(data.values.astype(np.float64), full_matrices=False)\n",
    "s[2:] = 0\n",
    "new_a = np.dot(U, np.dot(np.diag(s), Vh))\n",
    "for i in range(train.shape[1]):\n",
    "    plt.plot(range(1971,train.shape[0]+1971), train[:,i])\n",
    "    plt.plot(range(1971,train.shape[0]+1971), new_a[:,i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Mode Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydmd import DMD\n",
    "import scipy.integrate\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib\n",
    "\n",
    "SMALL_SIZE = 15\n",
    "matplotlib.rc('font', size=SMALL_SIZE)\n",
    "matplotlib.rc('axes', titlesize=SMALL_SIZE)\n",
    "dmd = DMD(svd_rank=2)\n",
    "dmd.fit(train[:-7,:].T)\n",
    "output = dmd.reconstructed_data\n",
    "A = dmd.atilde\n",
    "#plt.plot(range(0,44), train[:,3])\n",
    "#plt.plot(range(0,44), np.real(output.T[:,3]))\n",
    "#plt.show()\n",
    "#mae = mean_squared_error(sklearn.preprocessing.scale(train[:,3]), sklearn.preprocessing.scale(np.real(output.T[:,3])))\n",
    "dmd.dmd_time['tend'] += 7\n",
    "output = dmd.reconstructed_data\n",
    "error_list = []\n",
    "i = 3\n",
    "plt.figure(figsize=(10,6))\n",
    "outplot = np.real(output.T[:,i])\n",
    "plt.plot(range(1971,train.shape[0]+1971), train[:,i])\n",
    "plt.plot(range(1971,train.shape[0]+1971-7), outplot[:-7])\n",
    "plt.plot(range(train.shape[0]+1971-7,train.shape[0]+1971), outplot[-7:], linestyle='--')\n",
    "plt.title('Dynamic Mode Decomposition Prediction of ' + data.columns.values[i])\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Ecological Footprint (Earths)')\n",
    "#plt.savefig(data.columns.values[i] + ' dmd.pdf', bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "mae = (mean_squared_error(train[-7:,i], np.real(output.T)[-7:,i]))\n",
    "error_list.append(mae)\n",
    "print(error_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAR example\n",
    "import statsmodels.api as sm\n",
    "# contrived dataset with dependency\n",
    "\n",
    "print(data['ecological footprint'].values)\n",
    "# Fit the model\n",
    "mod = sm.tsa.statespace.SARIMAX(data['ecological footprint'].values, trend='c', order=(1,1,1))\n",
    "res = mod.fit(disp=False)\n",
    "print(res.summary())\n",
    "\n",
    "# fit model\n",
    "model = VAR(train[:-14,:])\n",
    "model_fit = model.fit()\n",
    "# make prediction\n",
    "yhat = model_fit.forecast(model_fit.y, steps=14)\n",
    "#print(yhat)\n",
    "print(model_fit.score(model_fit.y, steps = 14))\n",
    "for i in range(train.shape[1]):\n",
    "    plt.plot(range(1,15), train[-14:,i])\n",
    "    plt.plot(range(1,15), yhat[:,i])\n",
    "    plt.legend(['actual','pred'])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@author: Maziar Raissi\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "#from plotting import newfig, savefig\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    \n",
    "    # function that returns dx/dt\n",
    "    def f(x,t): # x is 3 x 1\n",
    "        J0 = 2.5\n",
    "        k1 = 100.0\n",
    "        k2 = 6.0\n",
    "        k3 = 16.0\n",
    "        k4 = 100.0\n",
    "        k5 = 1.28\n",
    "        k6 = 12.0\n",
    "        k = 1.8\n",
    "        kappa = 13.0\n",
    "        q = 4\n",
    "        K1 = 0.52\n",
    "        psi = 0.1\n",
    "        N = 1.0\n",
    "        A = 4.0\n",
    "        \n",
    "        f1 = J0 - (k1*x[0]*x[5])/(1 + (x[5]/K1)**q)\n",
    "        f2 = 2*(k1*x[0]*x[5])/(1 + (x[5]/K1)**q) - k2*x[1]*(N-x[4]) - k6*x[1]*x[4]\n",
    "        f3 = k2*x[1]*(N-x[4]) - k3*x[2]*(A-x[5])\n",
    "        f4 = k3*x[2]*(A-x[5]) - k4*x[3]*x[4] - kappa*(x[3]-x[6])\n",
    "        f5 = k2*x[1]*(N-x[4]) - k4*x[3]*x[4] - k6*x[1]*x[4]\n",
    "        f6 = -2*(k1*x[0]*x[5])/(1 + (x[5]/K1)**q) + 2*k3*x[2]*(A-x[5]) - k5*x[5]\n",
    "        f7 = psi*kappa*(x[3]-x[6]) - k*x[6]\n",
    "        \n",
    "        f = np.array([f1,f2,f3,f4,f5,f6,f7])\n",
    "        return f\n",
    "        \n",
    "    # time points\n",
    "    t_star = np.arange(1971,2015,1)\n",
    "    \n",
    "    S1 = np.random.uniform(0.15,1.60,1)\n",
    "    S2 = np.random.uniform(0.19,2.16,1)\n",
    "    S3 = np.random.uniform(0.04,0.20,1)\n",
    "    S4 = np.random.uniform(0.10,0.35,1)\n",
    "    S5 = np.random.uniform(0.08,0.30,1)\n",
    "    S6 = np.random.uniform(0.14,2.67,1)\n",
    "    S7 = np.random.uniform(0.05,0.10,1)\n",
    "    \n",
    "    # initial condition\n",
    "    X_train = send_train()\n",
    "    x0 = np.array(X_train[0,:]).flatten()\n",
    "    \n",
    "    # solve ODE\n",
    "    #X_star = odeint(f, x0, t_star)\n",
    "    \n",
    "    noise = 0.00\n",
    "    \n",
    "    skip = 1\n",
    "    dt = 1\n",
    "    #X_train = X_star[0::skip,:]\n",
    "    X_train = send_train()\n",
    "    #X_train = X_train + noise*X_train.std(0)*np.random.randn(X_train.shape[0], X_train.shape[1])\n",
    "    \n",
    "    X_train = np.reshape(X_train, (1,X_train.shape[0],X_train.shape[1]))\n",
    "    print(X_train.shape)\n",
    "    layers = [19, 256, 19]\n",
    "    \n",
    "    M = 1\n",
    "    scheme = 'AM'\n",
    "    model = Multistep_NN(dt, X_train, layers, M, scheme)\n",
    "    \n",
    "    N_Iter = 50000\n",
    "    model.train(N_Iter)\n",
    "    \n",
    "    def learned_f(x,t):\n",
    "        f = model.predict_f(x[None,:])\n",
    "        return f.flatten()\n",
    "    print(x0.shape)\n",
    "    print(t_star.shape)\n",
    "    print(learned_f)\n",
    "    learned_X_star = odeint(learned_f, x0.astype(float), t_star.astype(float))\n",
    "    print(learned_X_star[:,-1])\n",
    "    ####### Plotting ##################\n",
    "    \n",
    "    plt.plot(t_star, learned_X_star[:,:])\n",
    "    \n",
    "    fig, ax = newfig(1.0, 1.55)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    gs0 = gridspec.GridSpec(3, 2)\n",
    "    gs0.update(top=0.95, bottom=0.35, left=0.1, right=0.95, hspace=0.5, wspace=0.3)\n",
    "    \n",
    "    ax = plt.subplot(gs0[0:1, 0:1])\n",
    "    ax.plot(t_star,X_star[:,0],'r-')\n",
    "    ax.plot(t_star,learned_X_star[:,0],'k--')\n",
    "    ax.set_xlabel('$t$')\n",
    "    ax.set_ylabel('$S_1$')\n",
    "    \n",
    "    ax = plt.subplot(gs0[0:1, 1:2])\n",
    "    ax.plot(t_star,X_star[:,1],'r-')\n",
    "    ax.plot(t_star,learned_X_star[:,1],'k--')\n",
    "    ax.set_xlabel('$t$')\n",
    "    ax.set_ylabel('$S_2$')\n",
    "\n",
    "    ax = plt.subplot(gs0[1:2, 0:1])\n",
    "    ax.plot(t_star,X_star[:,2],'r-')\n",
    "    ax.plot(t_star,learned_X_star[:,2],'k--')\n",
    "    ax.set_xlabel('$t$')\n",
    "    ax.set_ylabel('$S_3$')\n",
    "    \n",
    "    ax = plt.subplot(gs0[1:2, 1:2])\n",
    "    ax.plot(t_star,X_star[:,3],'r-')\n",
    "    ax.plot(t_star,learned_X_star[:,3],'k--')\n",
    "    ax.set_xlabel('$t$')\n",
    "    ax.set_ylabel('$S_4$')\n",
    "    \n",
    "    ax = plt.subplot(gs0[2:3, 0:1])\n",
    "    ax.plot(t_star,X_star[:,4],'r-')\n",
    "    ax.plot(t_star,learned_X_star[:,4],'k--')\n",
    "    ax.set_xlabel('$t$')\n",
    "    ax.set_ylabel('$S_5$')\n",
    "    \n",
    "    ax = plt.subplot(gs0[2:3, 1:2])\n",
    "    ax.plot(t_star,X_star[:,5],'r-')\n",
    "    ax.plot(t_star,learned_X_star[:,5],'k--')\n",
    "    ax.set_xlabel('$t$')\n",
    "    ax.set_ylabel('$S_6$')\n",
    "\n",
    "    gs1 = gridspec.GridSpec(1, 2)\n",
    "    gs1.update(top=0.25, bottom=0.105, left=0.325, right=0.7, hspace=0.5, wspace=0.3)\n",
    "    \n",
    "    ax = plt.subplot(gs1[0:1, 0:2])\n",
    "    ax.plot(t_star,X_star[:,6],'r-',label='Exact Dynamics')\n",
    "    ax.plot(t_star,learned_X_star[:,6],'k--',label='Learned Dynamics')\n",
    "    ax.set_xlabel('$t$')\n",
    "    ax.set_ylabel('$S_7$')\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.4), ncol=2, frameon=False)\n",
    "\n",
    "# savefig('./figures/Glycolytic', crop = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = send_train()\n",
    "#X_train = X_train + noise*X_train.std(0)*np.random.randn(X_train.shape[0], X_train.shape[1])\n",
    "    \n",
    "X_train = np.reshape(X_train, (1,X_train.shape[0],X_train.shape[1]))\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_star = np.arange(1970,2016,1)\n",
    "print(t_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
